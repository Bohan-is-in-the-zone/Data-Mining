{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Assignment 1: Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: The version of the sklearn is 1.0.2. Hence, the Adaboost result is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries for the data preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "raw_dataset = pd.read_csv(\"breast-cancer-wisconsin.csv\")\n",
    "# First, replace the ? with np.nan value\n",
    "raw_dataset = raw_dataset.replace(\"?\", np.nan)\n",
    "\"\"\"\n",
    "Split the dataset into data(columns without class) and target(class column)\n",
    "1. Store the class column\n",
    "2. Remove the class column from the dataset\n",
    "3. Store the new dataset to a new address\n",
    "\"\"\"\n",
    "target = raw_dataset['class']\n",
    "del raw_dataset['class']\n",
    "data = raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset\n",
    "#FILL THE MISSING VALUE\n",
    "# https://towardsdatascience.com/imputing-missing-values-using-the-simpleimputer-class-in-sklearn-99706afaff46\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "imputer = imputer.fit(data)\n",
    "data = imputer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALISING THE DATA\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "data = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHANGING CLASS VALUE\n",
    "target = target.replace(\"class1\", 0)\n",
    "target = target.replace(\"class2\", 1)\n",
    "target = target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444,0.0000,0.0000,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.4444,0.3333,0.3333,0.4444,0.6667,1.0000,0.2222,0.1111,0.0000,0\n",
      "0.2222,0.0000,0.0000,0.0000,0.1111,0.1111,0.2222,0.0000,0.0000,0\n",
      "0.5556,0.7778,0.7778,0.0000,0.2222,0.3333,0.2222,0.6667,0.0000,0\n",
      "0.3333,0.0000,0.0000,0.2222,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.7778,1.0000,1.0000,0.7778,0.6667,1.0000,0.8889,0.6667,0.0000,1\n",
      "0.0000,0.0000,0.0000,0.0000,0.1111,1.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.1111,0.0000,0.1111,0.0000,0.2222,0.0000,0.0000,0\n",
      "0.1111,0.0000,0.0000,0.0000,0.1111,0.0000,0.0000,0.0000,0.4444,0\n",
      "0.3333,0.1111,0.0000,0.0000,0.1111,0.0000,0.1111,0.0000,0.0000,0\n"
     ]
    }
   ],
   "source": [
    "# Print first ten rows of pre-processed dataset to 4 decimal places as per assignment spec\n",
    "# A function is provided to assist\n",
    "\n",
    "def print_data(X, y, n_rows=10):\n",
    "    \"\"\"Takes a numpy data array and target and prints the first ten rows.\n",
    "    \n",
    "    Arguments:\n",
    "        X: numpy array of shape (n_examples, n_features)\n",
    "        y: numpy array of shape (n_examples)\n",
    "        n_rows: numpy of rows to print\n",
    "    \"\"\"\n",
    "    for example_num in range(n_rows):\n",
    "        for feature in X[example_num]:\n",
    "            print(\"{:.4f}\".format(feature), end=\",\")\n",
    "\n",
    "        if example_num == len(X)-1:\n",
    "            print(y[example_num],end=\"\")\n",
    "        else:\n",
    "            print(y[example_num])\n",
    "            \n",
    "            \n",
    "print_data(data, target, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Cross-validation without parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the 10 fold stratified cross-validation\n",
    "cvKFold=StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "# The stratified folds from cvKFold should be provided to the classifiers\n",
    "# Pass the cvKFold object to cv parameter via ED forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection imporct cross_val_score\n",
    "def logregClassifier(X, y):\n",
    "    logreg = LogisticRegression()\n",
    "    scores = cross_val_score(logreg, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naïve Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "def nbClassifier(X, y):\n",
    "    nb = GaussianNB()\n",
    "    scores = cross_val_score(nb, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def dtClassifier(X, y):\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "    scores = cross_val_score(tree, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles: Bagging, Ada Boost and Gradient Boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def bagDTClassifier(X, y, n_estimators, max_samples, max_depth):\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                                  max_depth=max_depth, random_state=0)\n",
    "    bag_clf = BaggingClassifier(tree, n_estimators=n_estimators, \n",
    "                                max_samples=max_samples, bootstrap=True, random_state=0)\n",
    "    scores = cross_val_score(bag_clf, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "def adaDTClassifier(X, y, n_estimators, learning_rate, max_depth):\n",
    "    tree = DecisionTreeClassifier(criterion='entropy', \n",
    "                                  max_depth=max_depth, random_state=0)\n",
    "    ada_clf = AdaBoostClassifier(tree, n_estimators=n_estimators, \n",
    "                                 learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(ada_clf, X, y, cv=cvKFold)\n",
    "    \n",
    "    return scores.mean()\n",
    "\n",
    "def gbClassifier(X, y, n_estimators, learning_rate):\n",
    "    gb_clf = GradientBoostingClassifier(n_estimators=n_estimators, \n",
    "                                        learning_rate=learning_rate, random_state=0)\n",
    "    scores = cross_val_score(gb_clf, X, y, cv=cvKFold) \n",
    "    \n",
    "    return scores.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogR average cross-validation accuracy: 0.9642\n",
      "NB average cross-validation accuracy:0.9585\n",
      "DT average cross-validation accuracy:0.9385\n",
      "Bagging average cross-validation accuracy:0.9571\n",
      "AdaBoost average cross-validation accuracy:0.9542\n",
      "GB average cross-validation accuracy:0.9613\n"
     ]
    }
   ],
   "source": [
    "# Parameters for Part 1:\n",
    "\n",
    "#Bagging\n",
    "bag_n_estimators = 60\n",
    "bag_max_samples = 100\n",
    "bag_max_depth = 6\n",
    "\n",
    "#AdaBoost\n",
    "ada_n_estimators = 60\n",
    "ada_learning_rate = 0.5\n",
    "ada_bag_max_depth = 6\n",
    "\n",
    "#GB\n",
    "gb_n_estimators = 60\n",
    "gb_learning_rate = 0.5\n",
    "\n",
    "logR_acc = logregClassifier(data, target)\n",
    "NB_acc = nbClassifier(data, target)\n",
    "DT_acc = dtClassifier(data, target)\n",
    "bagging_acc = bagDTClassifier(data, target, bag_n_estimators, \n",
    "                              bag_max_samples, bag_max_depth)\n",
    "ada_acc = adaDTClassifier(data, target, ada_n_estimators, \n",
    "                          ada_learning_rate, ada_bag_max_depth)\n",
    "gb_acc = gbClassifier(data, target, gb_n_estimators, gb_learning_rate)\n",
    "\n",
    "# Print results for each classifier in part 1 to 4 decimal places here:\n",
    "print(\"LogR average cross-validation accuracy: {:.4f}\".format(logR_acc))\n",
    "print(\"NB average cross-validation accuracy:{:.4f}\".format(NB_acc))\n",
    "print(\"DT average cross-validation accuracy:{:.4f}\".format(DT_acc))\n",
    "print(\"Bagging average cross-validation accuracy:{:.4f}\".format(bagging_acc))\n",
    "print(\"AdaBoost average cross-validation accuracy:{:.4f}\".format(ada_acc))\n",
    "print(\"GB average cross-validation accuracy:{:.4f}\".format(gb_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cross-validation with parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "k = [1, 3, 5, 7, 9]\n",
    "p = [1, 2]\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def bestKNNClassifier(X, y):\n",
    "    #split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=target, random_state=0)\n",
    "    \n",
    "    param_grid = {'n_neighbors': k,\n",
    "              'p': p}\n",
    "    grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "# You should use SVC from sklearn.svm with kernel set to 'rbf'\n",
    "C = [0.01, 0.1, 1, 5, 15] \n",
    "gamma = [0.01, 0.1, 1, 10, 50]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "def bestSVMClassifier(X, y):\n",
    "    #split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=target, random_state=0)\n",
    "    \n",
    "    param_grid = {'C': C,\n",
    "              'gamma': gamma}\n",
    "    grid_search = GridSearchCV(SVC(kernel=\"rbf\"), param_grid, cv=cvKFold,\n",
    "                          return_train_score=True)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# You should use RandomForestClassifier from sklearn.ensemble with information gain and max_features set to ‘sqrt’.\n",
    "n_estimators = [10, 30, 60, 100, 150]\n",
    "max_leaf_nodes = [6, 12, 18]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def bestRFClassifier(X, y):\n",
    "    #split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=target, random_state=0)\n",
    "    \n",
    "    param_grid = {'n_estimators': n_estimators,\n",
    "              'max_leaf_nodes': max_leaf_nodes} \n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(criterion='entropy', max_features='sqrt', random_state=0), \n",
    "                               param_grid, cv=cvKFold, return_train_score=True)\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    return grid_search       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best k: 3\n",
      "KNN best p: 1\n",
      "KNN cross-validation accuracy: 0.9695\n",
      "KNN test set accuracy: 0.9543\n",
      "\n",
      "SVM best C: 5.0000\n",
      "SVM best gamma: 0.1000\n",
      "SVM cross-validation accuracy: 0.9676\n",
      "SVM test set accuracy: 0.9714\n",
      "\n",
      "RF best n_estimators: 150\n",
      "RF best max_leaf_nodes: 6\n",
      "RF cross-validation accuracy: 0.9675\n",
      "RF test set accuracy: 0.9657\n",
      "RF test set macro average F1: 0.9628\n",
      "RF test set weighted average F1: 0.9661\n"
     ]
    }
   ],
   "source": [
    "# Perform Grid Search with 10-fold stratified cross-validation (GridSearchCV in sklearn). \n",
    "# The stratified folds from cvKFold should be provided to GridSearchV\n",
    "\n",
    "# This should include using train_test_split from sklearn.model_selection with stratification and random_state=0\n",
    "# Print results for each classifier here. All results should be printed to 4 decimal places except for\n",
    "# \"k\", \"p\", n_estimators\" and \"max_leaf_nodes\" which should be printed as integers.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, stratify=target, random_state=0)\n",
    "\n",
    "#retrieve the result\n",
    "knn_result = bestKNNClassifier(data, target)\n",
    "svm_result = bestSVMClassifier(data, target)\n",
    "rf_result = bestRFClassifier(data, target)\n",
    "\n",
    "print(\"KNN best k:\", knn_result.best_params_['n_neighbors'])\n",
    "print(\"KNN best p:\", knn_result.best_params_['p'])\n",
    "print(\"KNN cross-validation accuracy: {:.4f}\".format(knn_result.best_score_))\n",
    "print(\"KNN test set accuracy: {:.4f}\".format(knn_result.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"SVM best C: {:.4f}\".format(svm_result.best_params_['C']))\n",
    "print(\"SVM best gamma: {:.4f}\".format(svm_result.best_params_['gamma']))\n",
    "print(\"SVM cross-validation accuracy: {:.4f}\".format(svm_result.best_score_))\n",
    "print(\"SVM test set accuracy: {:.4f}\".format(svm_result.score(X_test, y_test)))\n",
    "\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_pred = rf_result.predict(X_test)\n",
    "print(\"RF best n_estimators:\", rf_result.best_params_['n_estimators'])\n",
    "print(\"RF best max_leaf_nodes:\", rf_result.best_params_['max_leaf_nodes'])\n",
    "print(\"RF cross-validation accuracy: {:.4f}\".format(rf_result.best_score_))\n",
    "print(\"RF test set accuracy: {:.4f}\".format(rf_result.score(X_test, y_test)))\n",
    "print(\"RF test set macro average F1: {:.4f}\"\n",
    "      .format(f1_score(y_test, y_pred, average='macro')))\n",
    "print(\"RF test set weighted average F1: {:.4f}\"\n",
    "      .format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
